{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:17:15.755148Z",
     "start_time": "2020-04-14T19:17:15.734853Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:27:50.038995Z",
     "start_time": "2020-04-14T19:27:49.986485Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn import decomposition, linear_model, model_selection, ensemble\n",
    "import warnings\n",
    "from rpy2 import robjects\n",
    "import rpy2.robjects.numpy2ri\n",
    "import multiprocessing\n",
    "import glmnet_python\n",
    "from glmnet import glmnet\n",
    "from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet\n",
    "from cvglmnetPredict import cvglmnetPredict\n",
    "import pickle\n",
    "import scipy\n",
    "# tqdm is not a necessary dependency\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "robjects.r.library(\"irr\")\n",
    "robjects.r.library(\"psych\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:38:36.749857Z",
     "start_time": "2020-04-14T19:38:36.224954Z"
    }
   },
   "outputs": [],
   "source": [
    "hcp_behavioral_data = pd.read_csv('../data/unrestricted_HCP_behavioral_data.csv')  # load unrestricted HCP behavioral data, available for download at http://db.humanconnectome.org\n",
    "hcp_behavioral_data_restricted = pd.read_csv('../data/restricted_HCP_behavioral_data.csv')  # load restricted HCP behavioral data, available for download at http://db.humanconnectome.org\n",
    "hcp_factors = pd.read_csv('../data/factors.csv')\n",
    "hcp_behavioral_data = hcp_behavioral_data.merge(hcp_factors, on='Subject', how='left')\n",
    "hcp_behavioral_data = hcp_behavioral_data.merge(hcp_behavioral_data_restricted[['Subject', 'ASR_Intn_T', 'ASR_Extn_T', 'ASR_Attn_Pct']], on='Subject', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:38:38.207545Z",
     "start_time": "2020-04-14T19:38:37.582590Z"
    }
   },
   "outputs": [],
   "source": [
    "sess1_matrix = pickle.load(open('../data/sess1_matrix.pickle', 'rb'))\n",
    "sess2_matrix = pickle.load(open('../data/sess2_matrix.pickle', 'rb'))\n",
    "folds = pickle.load(open('../data/folds.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:40:42.660709Z",
     "start_time": "2020-04-14T19:40:42.603206Z"
    }
   },
   "outputs": [],
   "source": [
    "ALL_PHEN = ['GenExec','ProcSpeed','PMAT24_A_CR','ASR_Extn_T',\n",
    "            'ASR_Intn_T','ASR_Attn_Pct','NEOFAC_O','NEOFAC_C','NEOFAC_E',\n",
    "            'NEOFAC_A','NEOFAC_N',\n",
    "            'DDisc_AUC_40K','ProcSpeed_Unadj','PicSeq_Unadj',\n",
    "            'CardSort_Unadj','Flanker_Unadj','ListSort_Unadj',\n",
    "            'ReadEng_Unadj','PicVocab_Unadj','SCPT_SEN',\n",
    "            'SCPT_SPEC','IWRD_TOT','VSPLOT_TC',\n",
    "            'MMSE_Score','PSQI_Score','Endurance_Unadj','GaitSpeed_Comp',\n",
    "            'Dexterity_Unadj','Strength_Unadj','Odor_Unadj',\n",
    "            'PainInterf_Tscore','Taste_Unadj','Mars_Final',\n",
    "            'Emotion_Task_Face_Acc','Language_Task_Math_Avg_Difficulty_Level',\n",
    "            'Language_Task_Story_Avg_Difficulty_Level','Social_Task_Perc_Random',\n",
    "            'Social_Task_Perc_TOM','WM_Task_Acc','ER40_CR','ER40ANG',\n",
    "            'ER40FEAR','ER40NOE','ER40SAD','AngAffect_Unadj',\n",
    "            'AngHostil_Unadj','AngAggr_Unadj','FearAffect_Unadj',\n",
    "            'FearSomat_Unadj','Sadness_Unadj','LifeSatisf_Unadj',\n",
    "            'MeanPurp_Unadj','PosAffect_Unadj','Friendship_Unadj',\n",
    "            'Loneliness_Unadj','PercHostil_Unadj','PercReject_Unadj',\n",
    "            'EmotSupp_Unadj','InstruSupp_Unadj','PercStress_Unadj','SelfEff_Unadj']\n",
    "\n",
    "cols = ALL_PHEN\n",
    "cols.append('Subject')\n",
    "rest_data_df = hcp_behavioral_data[cols]\n",
    "rest_data_df = rest_data_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Cross Validation Helper Functions\n",
    "\n",
    "These functions define the cross validation scheme described in the paper, used to compute model ICCs and Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:49:36.505104Z",
     "start_time": "2020-04-14T19:49:36.440321Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_message(msg_id, fold_i, sess_i):\n",
    "    fold_i = int(fold_i)\n",
    "    if msg_id == 0:\n",
    "        print(f'CV Fold: {fold_i+1:<10} Preprocessing session {sess_i} data...')\n",
    "    elif msg_id == 1:\n",
    "        print(f'CV Fold: {fold_i+1:<10} Fitting session {sess_i} model...')\n",
    "    elif msg_id == 2:\n",
    "        print(f'CV Fold: {fold_i+1:<10} Predicting session {sess_i} model...')\n",
    "    elif msg_id == 3:\n",
    "        print(f'CV Fold: {fold_i+1:<10} Scoring session {sess_i} model...')\n",
    "    elif msg_id == 4:\n",
    "        print('='*55)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "# for each fold,\n",
    "# train on session 1\n",
    "#   predict on session 1 and 2\n",
    "#   compute ICC of predictions\n",
    "# train on session 2\n",
    "#   predict on session 2 and 1\n",
    "#   compute ICC of predictions\n",
    "# final results is average of 10 ICCs\n",
    "\n",
    "def cross_validate(sess1_X, sess2_X, Y, \n",
    "                   model_trainer, model_predictor, model_scorer, \n",
    "                   center_X, scale_X, center_Y, scale_Y, \n",
    "                   folds, enable_tqdm=False, verbose=True, tqdm_desc=''):\n",
    "    if enable_tqdm: print()\n",
    "    folds_scores = []\n",
    "    sess1_folds_preds = []\n",
    "    sess2_folds_preds = []\n",
    "    models = []\n",
    "    fold_id_iter = tqdm(np.arange(folds.min(), folds.max()+1), desc=tqdm_desc, ncols='50%') if enable_tqdm else np.arange(folds.min(), folds.max()+1)\n",
    "    for fold_id in fold_id_iter:\n",
    "        train_idxs = np.argwhere(folds != fold_id).flatten()\n",
    "        test_idxs = np.argwhere(folds == fold_id).flatten()\n",
    "        \n",
    "        train_Y_mu = Y[train_idxs].mean() if center_Y else 0\n",
    "        train_Y_sd = Y[train_idxs].std() if scale_Y else 1\n",
    "        Y_std = (Y - train_Y_mu) / train_Y_sd\n",
    "        \n",
    "        # model for session 1\n",
    "        if verbose: print_message(0, fold_id, 1)\n",
    "        train_X_mu = sess1_X[train_idxs, :].mean(0) if center_X else 0 \n",
    "        train_X_sd = sess1_X[train_idxs, :].std(0) if scale_X else 1\n",
    "        sess1_X_std = (sess1_X - train_X_mu) / train_X_sd\n",
    "        sess2_X_std = (sess2_X - train_X_mu) / train_X_sd\n",
    "        if verbose: print_message(1, fold_id, 1)\n",
    "        sess1_model = model_trainer(sess1_X_std[train_idxs, :], Y_std[train_idxs])\n",
    "        if verbose: print_message(2, fold_id, 1)\n",
    "        sess1_preds = model_predictor(sess1_X_std[test_idxs, :], sess1_model)\n",
    "        sess2_preds = model_predictor(sess2_X_std[test_idxs, :], sess1_model)\n",
    "        if verbose: print_message(3, fold_id, 1)\n",
    "        sess1_model_scores = model_scorer(sess1_preds, sess2_preds, Y_std[test_idxs])\n",
    "        sess1_folds_preds.append((sess1_preds, sess2_preds, Y_std[test_idxs]))\n",
    "        \n",
    "        # model for session 2\n",
    "        if verbose: print_message(0, fold_id, 2)\n",
    "        train_X_mu = sess2_X[train_idxs, :].mean(0) if center_X else 0 \n",
    "        train_X_sd = sess2_X[train_idxs, :].std(0) if scale_X else 1\n",
    "        sess1_X_std = (sess1_X - train_X_mu) / train_X_sd\n",
    "        sess2_X_std = (sess2_X - train_X_mu) / train_X_sd\n",
    "        if verbose: print_message(1, fold_id, 2)\n",
    "        sess2_model = model_trainer(sess2_X_std[train_idxs, :], Y_std[train_idxs])\n",
    "        if verbose: print_message(2, fold_id, 2)\n",
    "        sess1_preds = model_predictor(sess1_X_std[test_idxs, :], sess2_model)\n",
    "        sess2_preds = model_predictor(sess2_X_std[test_idxs, :], sess2_model)\n",
    "        if verbose: print_message(3, fold_id, 2)\n",
    "        sess2_model_scores = model_scorer(sess1_preds, sess2_preds, Y_std[test_idxs])\n",
    "        sess2_folds_preds.append((sess1_preds, sess2_preds, Y_std[test_idxs]))\n",
    "        \n",
    "        folds_scores.append((sess1_model_scores, sess2_model_scores)) \n",
    "        models.append((sess1_model, sess2_model))\n",
    "        if verbose: print_message(4, fold_id, np.float('nan'))\n",
    "            \n",
    "    return folds_scores, sess1_folds_preds, sess2_folds_preds, []  # can return models as last value, skipped to save disk-space\n",
    "\n",
    "\n",
    "def default_scorer(preds1, preds2, trueY):\n",
    "    with warnings.catch_warnings():\n",
    "        # we expect RRuntimeWarning from ICC calling lmer for random effects modelling\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        preds_mat2D = np.stack((preds1, preds2)).T\n",
    "        rpy2.robjects.numpy2ri.activate()\n",
    "        icc_2_1 = robjects.r.ICC(preds_mat2D)[0][1][1]\n",
    "    # score per fold is \n",
    "    # (ICC, pred_corr, sess1_acc, sess2_acc)\n",
    "    return icc_2_1, stats.pearsonr(preds1, preds2)[0], stats.pearsonr(preds1, trueY)[0], stats.pearsonr(preds2, trueY)[0]\n",
    "\n",
    "\n",
    "def multi_cross_validate(phen_names, cross_validate_fn, sess1_X=sess1_matrix, sess2_X=sess2_matrix, n_procs=1):    \n",
    "    if __name__ == '__main__':\n",
    "        if n_procs > 1:\n",
    "            pool = multiprocessing.Pool(n_procs)\n",
    "            results = pool.starmap(cross_validate_fn, [(sess1_X, sess2_X, rest_data_df[[phen]].values.flatten(), f'({int(phen_idx/4)}) {phen}') for phen_idx, phen in enumerate(phen_names)])\n",
    "            pool.close()\n",
    "            return dict(zip(phen_names, results))\n",
    "        else:\n",
    "            results = []\n",
    "            for phen_idx, phen in enumerate(phen_names):\n",
    "                results.append(cross_validate_fn(sess1_X, sess2_X, rest_data_df[[phen]].values.flatten(), f'({int(phen_idx)}) {phen}'))\n",
    "            return dict(zip(phen_names, results))\n",
    "    else:\n",
    "        raise Exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# BBS-75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def BBS_trainer(X, Y, n_components=75):\n",
    "        # fit pca\n",
    "        pca_model = decomposition.PCA(n_components=n_components, random_state=42).fit(X)\n",
    "        # dimension reduce\n",
    "        X_transformed = pca_model.transform(X)\n",
    "        # fit OLS model\n",
    "        lr_model = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "        lr_model.fit(X_transformed, Y)\n",
    "        return pca_model, lr_model\n",
    "    \n",
    "    \n",
    "def BBS_predictor(X, model):\n",
    "    pca_model, lr_model = model\n",
    "    X_transformed = pca_model.transform(X)\n",
    "    return lr_model.predict(X_transformed)\n",
    "\n",
    "def BBS_cross_validate_fn(sess1_X, sess2_X, Y, tqdm_desc): \n",
    "    return cross_validate(sess1_X, sess2_X, Y,\n",
    "                          BBS_trainer, BBS_predictor, default_scorer, \n",
    "                          center_X=True, scale_X=False, center_Y=False, scale_Y=False, \n",
    "                          folds=folds, enable_tqdm=False, verbose=False, tqdm_desc=tqdm_desc)\n",
    "\n",
    "BBS_results = multi_cross_validate(ALL_PHEN, BBS_cross_validate_fn, n_procs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(BBS_results, open('./saved_models/BBS75_results.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# BBS-CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BBSEstimator:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.pca_model = None\n",
    "        self.linear_model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        pca_model = decomposition.PCA(n_components=self.n_components, random_state=42).fit(X)\n",
    "        X_transformed = pca_model.transform(X)\n",
    "        lr_model = linear_model.LinearRegression(fit_intercept=True, normalize=False).fit(X_transformed, y)\n",
    "        self.pca_model = pca_model\n",
    "        self.linear_model = lr_model\n",
    "        \n",
    "    def get_params(self, deep = False):\n",
    "        return {'n_components': self.n_components}\n",
    "                #'pca_model': self.pca_model,\n",
    "                #'linear_model': self.linear_model}\n",
    "\n",
    "\n",
    "def BBS_scorer(estimator, X, y):\n",
    "    pca_model = estimator.pca_model\n",
    "    linear_model = estimator.linear_model\n",
    "    return stats.pearsonr(linear_model.predict(pca_model.transform(X)), y)[0]\n",
    "\n",
    "\n",
    "def BBSCV_trainer(X, Y, n_components=[25, 75, 150, 250]):\n",
    "    comp_scores = []\n",
    "    for n_comp in n_components:\n",
    "        scores = model_selection.cross_val_score(BBSEstimator(n_components=n_comp), X, Y, cv=5, scoring=BBS_scorer)\n",
    "        comp_scores.append(np.nanmean(scores))\n",
    "    \n",
    "    optimal_n_components_idx = comp_scores.index(max(comp_scores))\n",
    "    opt_n_components = n_components[optimal_n_components_idx]\n",
    "    # fit pca\n",
    "    pca_model = decomposition.PCA(n_components=opt_n_components, random_state=42).fit(X)\n",
    "    # fit OLS model\n",
    "    lr_model = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "    lr_model.fit(pca_model.transform(X), Y)\n",
    "    return pca_model, lr_model\n",
    "    \n",
    "    \n",
    "def BBSCV_predictor(X, model):\n",
    "    pca_model, lr_model = model\n",
    "    return lr_model.predict(pca_model.transform(X))\n",
    "\n",
    "def BBSCV_cross_validate_fn(sess1_X, sess2_X, Y, tqdm_desc): \n",
    "    return cross_validate(sess1_X, sess2_X, Y,\n",
    "                          BBSCV_trainer, BBSCV_predictor, default_scorer, \n",
    "                          center_X=True, scale_X=False, center_Y=False, scale_Y=False, \n",
    "                          folds=folds, enable_tqdm=True, verbose=False, tqdm_desc=tqdm_desc)\n",
    "\n",
    "BBSCV_results = multi_cross_validate(ALL_PHEN, BBSCV_cross_validate_fn, n_procs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(BBSCV_results, open('./saved_models/BBSCV_results.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:50:12.473754Z",
     "start_time": "2020-04-14T19:49:38.795343Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def LASSO_trainer(X, Y, n_cv=5):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        ridge_model = cvglmnet(alpha=1, nlambda=100, x=X, y=Y, ptype = 'mse', nfolds=n_cv, parallel=True, standardize=False)\n",
    "    return ridge_model\n",
    "    \n",
    "def LASSO_predictor(X, model):\n",
    "    return cvglmnetPredict(model, newx=X, s='lambda_min').flatten()\n",
    "\n",
    "def LASSO_cross_validate_fn(sess1_X, sess2_X, Y, tqdm_desc): \n",
    "    return cross_validate(sess1_X, sess2_X, Y,\n",
    "                          LASSO_trainer, LASSO_predictor, default_scorer, \n",
    "                          center_X=False, scale_X=False, center_Y=False, scale_Y=False, \n",
    "                          folds=folds, enable_tqdm=True, verbose=False, tqdm_desc=tqdm_desc)\n",
    "\n",
    "Lasso_results = multi_cross_validate(ALL_PHEN, LASSO_cross_validate_fn, n_procs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(Lasso_results, open('./saved_models/Lasso_results.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Ridge_trainer(X, Y, n_cv=5):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        ridge_model = cvglmnet(alpha=0, nlambda=100, x=X, y=Y, ptype = 'mse', nfolds=n_cv, parallel=True, standardize=False)\n",
    "    return ridge_model\n",
    "    \n",
    "def Ridge_predictor(X, model):\n",
    "    return cvglmnetPredict(model, newx=X, s='lambda_min').flatten()\n",
    "\n",
    "def Ridge_cross_validate_fn(sess1_X, sess2_X, Y, tqdm_desc): \n",
    "    return cross_validate(sess1_X, sess2_X, Y,\n",
    "                          Ridge_trainer, Ridge_predictor, default_scorer, \n",
    "                          center_X=False, scale_X=False, center_Y=False, scale_Y=False, \n",
    "                          folds=folds, enable_tqdm=True, verbose=False, tqdm_desc=tqdm_desc)\n",
    "\n",
    "Ridge_results = multi_cross_validate(ALL_PHEN, Ridge_cross_validate_fn, n_procs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(Ridge_results, open('./saved_models/Ridge_results.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def EnetCV_trainer(X, Y, alphas=[0, 0.1, 0.325, 0.55, 0.775, 1]):\n",
    "    with warnings.catch_warnings():\n",
    "        comp_scores = []\n",
    "        for alpha in alphas:\n",
    "            model = cvglmnet(alpha=alpha, nlambda=100, x=X, y=Y, ptype = 'mse', nfolds=5, parallel=True, standardize=False)\n",
    "            preds = cvglmnetPredict(model, newx=X, s='lambda_min').flatten()\n",
    "            comp_scores.append(stats.pearsonr(preds, Y)[0])\n",
    "\n",
    "\n",
    "        optimal_n_components_idx = comp_scores.index(max(comp_scores))\n",
    "        opt_alpha = alphas[optimal_n_components_idx]\n",
    "        # fit pca\n",
    "        enet_model = cvglmnet(alpha=opt_alpha, nlambda=100, x=X, y=Y, ptype = 'mse', nfolds=5, parallel=True, standardize=False)\n",
    "    return enet_model\n",
    "    \n",
    "    \n",
    "def EnetCV_predictor(X, model):\n",
    "    return cvglmnetPredict(model, newx=X, s='lambda_min').flatten()\n",
    "\n",
    "def EnetCV_cross_validate_fn(sess1_X, sess2_X, Y, tqdm_desc): \n",
    "    return cross_validate(sess1_X, sess2_X, Y,\n",
    "                          EnetCV_trainer, EnetCV_predictor, default_scorer, \n",
    "                          center_X=False, scale_X=False, center_Y=False, scale_Y=False, \n",
    "                          folds=folds, enable_tqdm=True, verbose=False, tqdm_desc=tqdm_desc)\n",
    "\n",
    "EnetCV_results = multi_cross_validate(ALL_PHEN, EnetCV_cross_validate_fn, n_procs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(EnetCV_results, open('./saved_models/ENet_results.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# CPM Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:54:33.774982Z",
     "start_time": "2020-04-14T19:54:32.064532Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CPM Positive\n",
    "def CPM_trainer(X, Y, p_threshold=0.01, is_pos=True):\n",
    "    with warnings.catch_warnings():\n",
    "        # we expect pearsonr to throw PearsonRConstantInputWarning because of contant valued columns in X\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        pheno_corr_p = [stats.pearsonr(X[:, i], Y) for i in range(X.shape[1])]\n",
    "        X_corrs = np.array([x[0] for x in pheno_corr_p])\n",
    "        X_pvals = np.array([x[1] for x in pheno_corr_p])\n",
    "        # create masks for edges below p-threshold and split pos/neg correlations\n",
    "        keep_edges_pos = (X_corrs > 0) & (X_pvals < p_threshold)\n",
    "        keep_edges_neg = (X_corrs < 0) & (X_pvals < p_threshold)\n",
    "\n",
    "    # sum X entries with significant correlations with y\n",
    "    X_pos_edges_sum = X[:, keep_edges_pos].sum(1)\n",
    "    X_neg_edges_sum = X[:, keep_edges_neg].sum(1)\n",
    "    \n",
    "    X_train = X_pos_edges_sum.reshape(-1, 1) if is_pos else X_neg_edges_sum.reshape(-1, 1)\n",
    "    keep_edges = keep_edges_pos if is_pos else keep_edges_neg\n",
    "    model =  linear_model.LinearRegression(fit_intercept=True, normalize=False).fit(X_train, Y)\n",
    "    return model, keep_edges\n",
    "\n",
    "    \n",
    "def CPM_predictor(X, model):\n",
    "    lr_model, keep_edges = model\n",
    "    X_edges_sum = X[:, keep_edges].sum(1)\n",
    "    return lr_model.predict(X_edges_sum.reshape(-1, 1))\n",
    "\n",
    "\n",
    "def CPM_cross_validate_fn(sess1_X, sess2_X, Y, tqdm_desc): \n",
    "    return cross_validate(sess1_X, sess2_X, Y,\n",
    "                          CPM_trainer, CPM_predictor, default_scorer, \n",
    "                          center_X=False, scale_X=False, center_Y=False, scale_Y=False, \n",
    "                          folds=folds, enable_tqdm=True, verbose=False, tqdm_desc=tqdm_desc)\n",
    "\n",
    "CPM_pos_results = multi_cross_validate(ALL_PHEN, CPM_cross_validate_fn, n_procs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(CPM_pos_results, open('./saved_models/CPMPos_results.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# CPM Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CPM Positive\n",
    "def CPM_trainer(X, Y, p_threshold=0.01, is_pos=False):\n",
    "    with warnings.catch_warnings():\n",
    "        # we expect pearsonr to throw PearsonRConstantInputWarning because of contant valued columns in X\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        pheno_corr_p = [stats.pearsonr(X[:, i], Y) for i in range(X.shape[1])]\n",
    "        X_corrs = np.array([x[0] for x in pheno_corr_p])\n",
    "        X_pvals = np.array([x[1] for x in pheno_corr_p])\n",
    "        # create masks for edges below p-threshold and split pos/neg correlations\n",
    "        keep_edges_pos = (X_corrs > 0) & (X_pvals < p_threshold)\n",
    "        keep_edges_neg = (X_corrs < 0) & (X_pvals < p_threshold)\n",
    "\n",
    "    # sum X entries with significant correlations with y\n",
    "    X_pos_edges_sum = X[:, keep_edges_pos].sum(1)\n",
    "    X_neg_edges_sum = X[:, keep_edges_neg].sum(1)\n",
    "    \n",
    "    X_train = X_pos_edges_sum.reshape(-1, 1) if is_pos else X_neg_edges_sum.reshape(-1, 1)\n",
    "    keep_edges = keep_edges_pos if is_pos else keep_edges_neg\n",
    "    model =  linear_model.LinearRegression(fit_intercept=True, normalize=False).fit(X_train, Y)\n",
    "    return model, keep_edges\n",
    "\n",
    "    \n",
    "def CPM_predictor(X, model):\n",
    "    lr_model, keep_edges = model\n",
    "    X_edges_sum = X[:, keep_edges].sum(1)\n",
    "    return lr_model.predict(X_edges_sum.reshape(-1, 1))\n",
    "\n",
    "\n",
    "def CPM_cross_validate_fn(sess1_X, sess2_X, Y, tqdm_desc): \n",
    "    return cross_validate(sess1_X, sess2_X, Y,\n",
    "                          CPM_trainer, CPM_predictor, default_scorer, \n",
    "                          center_X=False, scale_X=False, center_Y=False, scale_Y=False, \n",
    "                          folds=folds, enable_tqdm=True, verbose=False, tqdm_desc=tqdm_desc)\n",
    "\n",
    "CPM_neg_results = multi_cross_validate(ALL_PHEN, CPM_cross_validate_fn, n_procs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(CPM_neg_results, open('./saved_models/CPMNeg_results.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVREstimator:\n",
    "    def __init__(self, C, eps):\n",
    "        self.C = C\n",
    "        self.eps = eps\n",
    "        self.svr_model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        with warnings.catch_warnings():\n",
    "            svr_model = svm.LinearSVR(C=self.C, epsilon=self.eps).fit(X, y)\n",
    "            self.svr_model = svr_model\n",
    "        \n",
    "    def get_params(self, deep = False):\n",
    "        return {'C': self.C,\n",
    "                'eps': self.eps}\n",
    "\n",
    "\n",
    "def SVR_scorer(estimator, X, y):\n",
    "    return stats.pearsonr(estimator.svr_model.predict(X), y)[0]\n",
    "\n",
    "\n",
    "def SVRCV_trainer(X, Y, c_eps_vals = [(0.1, 0.1), (0.1, 1), (0.1, 10), \n",
    "                                      (1, 0.1), (1, 1), (1, 10), \n",
    "                                      (10, 0.1), (10, 1), (10, 10)]):\n",
    "    comp_scores = []\n",
    "    for C, eps in c_eps_vals:\n",
    "        scores = model_selection.cross_val_score(SVREstimator(C=C, eps=eps), X, Y, cv=5, scoring=SVR_scorer, n_jobs=5)\n",
    "        comp_scores.append(np.nanmean(scores))\n",
    "    \n",
    "    optimal_n_components_idx = comp_scores.index(max(comp_scores))\n",
    "    opt_C, opt_eps = c_eps_vals[optimal_n_components_idx]\n",
    "    svr_model = svm.LinearSVR(C=opt_C, epsilon=opt_eps).fit(X, Y)\n",
    "    return svr_model\n",
    "    \n",
    "    \n",
    "def SVRCV_predictor(X, model):\n",
    "    return model.predict(X)\n",
    "\n",
    "def SVRCV_cross_validate_fn(sess1_X, sess2_X, Y, tqdm_desc): \n",
    "    return cross_validate(sess1_X, sess2_X, Y,\n",
    "                          SVRCV_trainer, SVRCV_predictor, default_scorer, \n",
    "                          center_X=False, scale_X=False, center_Y=True, scale_Y=True, \n",
    "                          folds=folds, enable_tqdm=False, verbose=False, tqdm_desc=tqdm_desc)\n",
    "\n",
    "SVRCV_results = multi_cross_validate(ALL_PHEN, SVRCV_cross_validate_fn, n_procs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(SVRCV_results, open('./saved_models/SVRLin_results.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR Non Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVREstimator:\n",
    "    def __init__(self, C, eps):\n",
    "        self.C = C\n",
    "        self.eps = eps\n",
    "        self.svr_model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        with warnings.catch_warnings():\n",
    "            svr_model = svm.SVR(kernel='rbf', C=self.C, epsilon=self.eps, cache_size=10000, max_iter=1000).fit(X, y)\n",
    "            self.svr_model = svr_model\n",
    "        \n",
    "    def get_params(self, deep = False):\n",
    "        return {'C': self.C,\n",
    "                'eps': self.eps}\n",
    "\n",
    "\n",
    "def SVR_scorer(estimator, X, y):\n",
    "    return stats.pearsonr(estimator.svr_model.predict(X), y)[0]\n",
    "\n",
    "\n",
    "def SVRCV_trainer(X, Y, c_eps_vals =  [(0.1, 0.1), (0.1, 1), (0.1, 10),\n",
    "                                       (1, 0.1), (1, 1), (1, 10),\n",
    "                                       (10, 0.1), (10, 1), (10, 10)]):\n",
    "    comp_scores = []\n",
    "    for C, eps in c_eps_vals:\n",
    "        scores = model_selection.cross_val_score(SVREstimator(C=C, eps=eps), X, Y, cv=5, scoring=SVR_scorer, n_jobs=5)\n",
    "        comp_scores.append(np.nanmean(scores))\n",
    "    \n",
    "    optimal_n_components_idx = comp_scores.index(max(comp_scores))\n",
    "    opt_C, opt_eps = c_eps_vals[optimal_n_components_idx]\n",
    "    svr_model = svm.LinearSVR(C=opt_C, epsilon=opt_eps).fit(X, Y)\n",
    "    return svr_model\n",
    "    \n",
    "    \n",
    "def SVRCV_predictor(X, model):\n",
    "    return model.predict(X)\n",
    "\n",
    "def SVRCV_cross_validate_fn(sess1_X, sess2_X, Y, tqdm_desc): \n",
    "    return cross_validate(sess1_X, sess2_X, Y,\n",
    "                          SVRCV_trainer, SVRCV_predictor, default_scorer, \n",
    "                          center_X=False, scale_X=False, center_Y=True, scale_Y=True, \n",
    "                          folds=folds, enable_tqdm=False, verbose=False, tqdm_desc=tqdm_desc)\n",
    "\n",
    "NonLinSVRCV_results = multi_cross_validate(ALL_PHEN, SVRCV_cross_validate_fn, n_procs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(NonLinSVRCV_results, open('./saved_models/SVRNonLin_results.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFEstimator:\n",
    "    def __init__(self, max_depth, max_features, rand_state):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.rand_state = rand_state\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        model =  ensemble.RandomForestRegressor(n_estimators=5000, criterion='mse', max_depth=self.max_depth, max_features=self.max_features, random_state=self.rand_state, n_jobs=-1).fit(X, y)\n",
    "        self.model = model\n",
    "        \n",
    "    def get_params(self, deep = False):\n",
    "        return {'max_depth': self.max_depth,\n",
    "                'max_features': self.max_features,\n",
    "                'rand_state':self.rand_state}\n",
    "\n",
    "\n",
    "def RF_scorer(estimator, X, y):\n",
    "    return stats.pearsonr(estimator.model.predict(X).flatten(), y)[0]\n",
    "\n",
    "\n",
    "def RFCV_trainer(X, Y, depth_features=[(3, 'sqrt'), (3, 0.01), (3, 0.1),\n",
    "                                       (9, 'sqrt'), (9, 0.01), (9, 0.1),\n",
    "                                       (27, 'sqrt'), (27, 0.01), (27, 0.1)]):\n",
    "    comp_scores = []\n",
    "    for rand_state, (depth, n_features) in enumerate(depth_features):\n",
    "        scores = model_selection.cross_val_score(RFEstimator(max_depth=depth, max_features=n_features, rand_state=rand_state), X, Y, cv=5, scoring=RF_scorer, n_jobs=5)\n",
    "        comp_scores.append(np.nanmean(scores))\n",
    "    \n",
    "    optimal_n_components_idx = comp_scores.index(max(comp_scores))\n",
    "    opt_depth, opt_features = depth_features[optimal_n_components_idx]\n",
    "    rf_model = ensemble.RandomForestRegressor(n_estimators=5000, criterion='mse', max_depth=opt_depth, max_features=opt_features, random_state=optimal_n_components_idx, n_jobs=25).fit(X, Y)\n",
    "    return rf_model\n",
    "    \n",
    "    \n",
    "def RFCV_predictor(X, model):\n",
    "    return model.predict(X).flatten()\n",
    "\n",
    "def RFCV_cross_validate_fn(sess1_X, sess2_X, Y, tqdm_desc): \n",
    "    return cross_validate(sess1_X, sess2_X, Y,\n",
    "                          RFCV_trainer, RFCV_predictor, default_scorer, \n",
    "                          center_X=False, scale_X=False, center_Y=False, scale_Y=False, \n",
    "                          folds=folds, enable_tqdm=False, verbose=False, tqdm_desc=tqdm_desc)\n",
    "\n",
    "RFCV_results = multi_cross_validate(ALL_PHEN, RFCV_cross_validate_fn, n_procs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T20:02:11.530383Z",
     "start_time": "2020-04-14T20:02:11.486339Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(RFCV_results, open('./saved_models/RF_results.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sripada_lab",
   "language": "python",
   "name": "sripada_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
